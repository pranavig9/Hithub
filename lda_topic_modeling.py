# -*- coding: utf-8 -*-
"""4701.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vhIt5b2WA4ADLCkEAxMllnNPO-2ejn43
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

poems_list = [
    "I'll be the first to say, 'I'm sorry' Now you got me feelin' sorry I showed you all my demons, all my lies Yet you played me like Atari Now it's like I'm lookin' in the mirror Hope you feel alright when you're in her I found a good boy and he's on my side You're just my eternal sunshine, sunshine",
    "I don't wanna fuck with your head It's breakin' my heart To keep breakin' yours again (Yours again, yours again, yours again) This situationship has to end But I just can't refuse I don't wanna break up again (Up again, up again, up again), baby",
    "We can't be friends But I'd like to just pretend You cling to your papers and pens Wait until you like me again Wait for your love Lo-love, I'll wait for your love I'll wait for your love Lo-love, I'll wait for your love",
    "But no matter how I try to (Mm) And no matter how I want to (Mm) And no matter how easy things could be if I did And no matter how guilty I still feel saying it I wish I hated you I wish that weren't true Wish there was worse to you I wish you were worse to me Yeah, I wish I hated you",
    "Fucked up, anxious, too much But I'll love you like you need me to Imperfect for you Messy, completely distressed But I'm not like that since I met you"
]

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
preprocessed_poems = []

for poem in poems_list:
    words = word_tokenize(poem.lower())
    filtered_words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]
    preprocessed_poems.append(" ".join(filtered_words))

vectorizer = CountVectorizer(max_features=1000)
X = vectorizer.fit_transform(preprocessed_poems)

num_topics = 20  # You can adjust this based on your requirement
lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
lda.fit(X)

feature_names = vectorizer.get_feature_names_out()
topic_words = []

for topic_idx, topic in enumerate(lda.components_):
    top_words_idx = topic.argsort()[:-21:-1]  # Selecting top 5 words for each topic
    top_words = [feature_names[i] for i in top_words_idx]
    topic_words.append(top_words)

representative_words = set()

for words in topic_words:
    representative_words.update(words)

print("Representative words:", representative_words)